{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ INSTALL DEPENDENCIES FIRST\n",
        "%pip install -q langchain transformers PyPDF2 faiss-cpu google-generativeai langchain-google-genai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcEGEHRgpmKy",
        "outputId": "4f4ea3fc-bd69-4123-d5cc-37e03fba79e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U langchain-community\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2b8Zm_uqW-d",
        "outputId": "e312920c-b80b-4ae8-fc4e-259e484ee990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.72)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.12)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKvwiYAZowJW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "6b35c54e-6683-43e1-fa7d-90141e8f6c7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1047727176.py:40: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = qa_chain({\"query\": query})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† *Answer from Gemini (LLM):*\n",
            "In most jurisdictions, a constable has the power to make arrests, but the specifics depend heavily on the location and their specific duties.  Constables' powers vary significantly depending on the country, state, or province.\n",
            "\n",
            "* **Some jurisdictions grant constables full police powers:**  In these areas, they can arrest for any crime, just like a police officer.\n",
            "\n",
            "* **Other jurisdictions limit their powers:**  Their arrest powers may be limited to specific situations, such as serving warrants or making arrests related to court proceedings (e.g., contempt of court). They might not have the authority to arrest for general criminal offenses.\n",
            "\n",
            "* **Some jurisdictions may not have constables at all:**  The position is obsolete or has been replaced by other law enforcement roles in some areas.\n",
            "\n",
            "\n",
            "To know for sure if a constable in *your* location can arrest you, you need to check the laws of that specific jurisdiction (country, state, province, etc.).  Searching online for \"[Your Location] constable arrest powers\" will likely provide relevant information.  If you're unsure about a particular situation, contacting a legal professional is advisable.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Setup Gemini\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "GOOGLE_API_KEY = 'AIzaSyDNo_VDkus7erQ-pkYd6fFQOVGaspFmzeI'  # Replace with your actual key\n",
        "PDF_PATH = '/content/20240716890312078.pdf'\n",
        "\n",
        "# --- GEMINI SETUP ---\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", google_api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "\n",
        "# --- LOAD PDF & CREATE VECTORSTORE ---\n",
        "reader = PdfReader(\"/content/20240716890312078.pdf\")\n",
        "raw_text = ''.join(page.extract_text() for page in reader.pages if page.extract_text())\n",
        "\n",
        "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_text(raw_text)\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "vectorstore = FAISS.from_texts(texts, embedding=embeddings)\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "# --- CONSTITUTION-AWARE QUERY FUNCTION ---\n",
        "def ask(query):\n",
        "    try:\n",
        "        result = qa_chain({\"query\": query})\n",
        "        answer = result[\"result\"].strip()\n",
        "        source_documents = result[\"source_documents\"]\n",
        "\n",
        "        # Check if answer is vague or clearly a hallucination\n",
        "        hallucination_clues = [\n",
        "            \"i don't know\", \"i cannot provide\", \"depends on the jurisdiction\",\n",
        "            \"laws vary\", \"not specified\", \"not clear\", \"no specific article\",\n",
        "            \"as an ai\", \"i'm unable\", \"could not determine\", \"unclear\"\n",
        "        ]\n",
        "        fallback_needed = (\n",
        "            not answer or\n",
        "            any(clue in answer.lower() for clue in hallucination_clues) or\n",
        "            not source_documents\n",
        "        )\n",
        "\n",
        "        if fallback_needed:\n",
        "            raise ValueError(\"Fallback triggered\")\n",
        "\n",
        "        # Extract article numbers (if found)\n",
        "        articles = []\n",
        "        for doc in source_documents:\n",
        "            matches = re.findall(r'Article\\s*(\\d+)', doc.page_content)\n",
        "            articles.extend(matches)\n",
        "\n",
        "        response = f\"üìò *Answer from Constitution PDF:*\\n{answer}\\n\\n\"\n",
        "        if articles:\n",
        "            response += f\"üìå *Supporting Article(s)*: {', '.join(sorted(set(articles)))}\"\n",
        "        else:\n",
        "            response += \"üìå *Supporting Article(s)*: Could not determine specific article.\"\n",
        "        return response\n",
        "\n",
        "    except Exception:\n",
        "        # Fallback to Gemini LLM directly\n",
        "        try:\n",
        "            response = model.generate_content(query)\n",
        "            return f\"üß† *Answer from Gemini (LLM):*\\n{response.text.strip()}\"\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Gemini failed: {e}\"\n",
        "print(ask(\"can a constable arrest me?\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask(\"can a constable arrest me?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "eUl-HgssrFHN",
        "outputId": "3df16dea-6f2e-4ed7-d188-05c04d549d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† *Answer from Gemini (LLM):*\n",
            "Yes, a constable can arrest you, but their powers vary depending on their jurisdiction and the specific circumstances.  Constables typically have powers of arrest similar to police officers, but this isn't universally true across all locations.  The exact extent of their powers is determined by state or local laws.\n",
            "\n",
            "For example, some constables might primarily serve legal documents, while others might have full police powers, including the power to make arrests for crimes they witness or have probable cause to believe have been committed.\n",
            "\n",
            "Therefore, if a constable arrests you, it's important to:\n",
            "\n",
            "* **Ask them what the charges are.**\n",
            "* **Ask to see their identification.**\n",
            "* **Remain calm and cooperate (unless you believe your rights are being violated).**\n",
            "* **Request a lawyer if you are arrested.**\n",
            "\n",
            "\n",
            "If you are unsure about the legality of an arrest by a constable, you should seek legal advice.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask(\"can a constable arrest me?answer in hindi\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDMHUJyVvpNi",
        "outputId": "4af8b5a5-5138-4beb-94d8-5f729ca959c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìò *Answer from Constitution PDF:*\n",
            "‡§π‡§æ‡§Å, ‡§è‡§ï ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§Ü‡§™‡§ï‡•ã ‡§ó‡§ø‡§∞‡§´‡•ç‡§§‡§æ‡§∞ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® ‡§ï‡•á‡§µ‡§≤ ‡§§‡§≠‡•Ä ‡§ú‡§¨ ‡§â‡§∏‡§ï‡•á ‡§™‡§æ‡§∏ ‡§ê‡§∏‡§æ ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§π‡•ã, ‡§ú‡•à‡§∏‡•á ‡§ï‡§ø ‡§Ü‡§™‡§ï‡•ã ‡§ï‡§ø‡§∏‡•Ä ‡§Ö‡§™‡§∞‡§æ‡§ß ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ó‡§ø‡§∞‡§´‡•ç‡§§‡§æ‡§∞ ‡§ï‡§∞‡§®‡§æ‡•§\n",
            "\n",
            "üìå *Supporting Article(s)*: Could not determine specific article.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose not to use the Gemini Pro model because it's a paid resource, and my aim was to create a solution that's fully free and accessible ‚Äî especially useful for early-stage development, proof-of-concept, or open-source projects.\n",
        "\n",
        "Additionally, LangChain does not support the free Gemini Flash model ‚Äî it only works with Gemini Pro. Since I wanted to leverage LangChain‚Äôs powerful chaining and document retrieval features, I architected the system to:\n",
        "\n",
        "Use LangChain with PDFs (retrieval-based answers).\n",
        "\n",
        "And direct Gemini Flash API for real-time answers.\n",
        "\n",
        "This hybrid approach gives me the best of both worlds ‚Äî free access to a strong LLM (via Gemini Flash), and smart document-based reasoning (via LangChain). Once scaling or funding is available, I can easily upgrade to Gemini Pro and integrate it into the LangChain pipeline.\n",
        "\n",
        "üîß Bonus (If they ask ‚ÄúWhat will change if you get Pro?‚Äù):\n",
        "If Gemini Pro becomes available, I can plug it directly into LangChain using ChatGoogleGenerativeAI, which will enable me to add memory, multi-step agents, and tool-using capabilities. That would make the system far more intelligent and conversational.\n",
        "*`italicized text`*"
      ],
      "metadata": {
        "id": "DfKCY4R-1P9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "kal ye dekh kii ans aa khn se raha hai aur q?llm ya"
      ],
      "metadata": {
        "id": "koME0uI1NTuu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90b53836"
      },
      "source": [
        "## Add error handling and edge cases\n",
        "\n",
        "### Subtask:\n",
        "Improve error handling within the `ask` function to gracefully manage issues like file not found, API errors, or problems during text processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92768280"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to modify the `ask` function to include more robust error handling as requested in the instructions. This involves adding `try...except` blocks around the PDF processing, RAG steps, and the final LLM fallback to catch specific exceptions and provide informative messages.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "58ff6fc9",
        "outputId": "3f97623b-70a6-465e-f54e-ed561d84666e"
      },
      "source": [
        "# Update the ask function with improved error handling\n",
        "def ask(query):\n",
        "    raw_text = \"\"\n",
        "    try:\n",
        "        # Attempt to load and process the PDF\n",
        "        reader = PdfReader(PDF_PATH)\n",
        "        raw_text = ''.join(page.extract_text() for page in reader.pages if page.extract_text())\n",
        "\n",
        "        if not raw_text:\n",
        "             raise ValueError(\"Could not extract text from PDF.\")\n",
        "\n",
        "        text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=1000, chunk_overlap=200)\n",
        "        texts = text_splitter.split_text(raw_text)\n",
        "\n",
        "        if not texts:\n",
        "            raise ValueError(\"Could not split text into chunks.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: PDF file not found at {PDF_PATH}. Attempting direct LLM fallback.\")\n",
        "        raw_text = None # Indicate that PDF processing failed\n",
        "    except Exception as e:\n",
        "        print(f\"Error during PDF processing: {e}. Attempting direct LLM fallback.\")\n",
        "        raw_text = None # Indicate that PDF processing failed\n",
        "\n",
        "\n",
        "    # Attempt RAG if PDF processing was successful\n",
        "    if raw_text is not None:\n",
        "        try:\n",
        "            embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "            vectorstore = FAISS.from_texts(texts, embedding=embeddings)\n",
        "            retriever = vectorstore.as_retriever()\n",
        "\n",
        "            qa_chain = RetrievalQA.from_chain_type(\n",
        "                llm=llm,\n",
        "                chain_type=\"stuff\",\n",
        "                retriever=retriever,\n",
        "                return_source_documents=True\n",
        "            )\n",
        "\n",
        "            result = qa_chain({\"query\": query})\n",
        "            answer = result[\"result\"].strip()\n",
        "            source_documents = result[\"source_documents\"]\n",
        "\n",
        "            # Check if answer is vague or clearly a hallucination\n",
        "            hallucination_clues = [\n",
        "                \"i don't know\", \"i cannot provide\", \"depends on the jurisdiction\",\n",
        "                \"laws vary\", \"not specified\", \"not clear\", \"no specific article\",\n",
        "                \"as an ai\", \"i'm unable\", \"could not determine\", \"unclear\"\n",
        "            ]\n",
        "            fallback_needed = (\n",
        "                not answer or\n",
        "                any(clue in answer.lower() for clue in hallucination_clues) or\n",
        "                not source_documents\n",
        "            )\n",
        "\n",
        "            if fallback_needed:\n",
        "                raise ValueError(\"Fallback triggered due to vague/hallucinated answer or no source documents.\")\n",
        "\n",
        "            # Extract article numbers (if found)\n",
        "            articles = []\n",
        "            for doc in source_documents:\n",
        "                matches = re.findall(r'Article\\s*(\\d+)', doc.page_content)\n",
        "                articles.extend(matches)\n",
        "\n",
        "            response = f\"üìò *Answer from Constitution PDF:*\\n{answer}\\n\\n\"\n",
        "            if articles:\n",
        "                response += f\"üìå *Supporting Article(s)*: {', '.join(sorted(set(articles)))}\"\n",
        "            else:\n",
        "                response += \"üìå *Supporting Article(s)*: Could not determine specific article.\"\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during RAG process: {e}. Attempting direct LLM fallback.\")\n",
        "\n",
        "\n",
        "    # Fallback to Gemini LLM directly if RAG failed or PDF processing failed\n",
        "    try:\n",
        "        response = model.generate_content(query)\n",
        "        return f\"üß† *Answer from Gemini (LLM):*\\n{response.text.strip()}\"\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Both RAG and Gemini LLM failed: {e}\"\n",
        "\n",
        "print(ask(\"can a constable arrest me?\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during RAG process: Fallback triggered due to vague/hallucinated answer or no source documents.. Attempting direct LLM fallback.\n",
            "üß† *Answer from Gemini (LLM):*\n",
            "The power of a constable to arrest you depends entirely on the jurisdiction (country, state/province, etc.).  Constables have varying levels of police power in different places.\n",
            "\n",
            "In some places, constables have full police powers and can arrest you for any crime they witness or have probable cause to believe you committed.  In other places, their powers are more limited, perhaps only allowing them to make arrests for specific offenses or in specific situations (e.g., serving warrants).  In still other places, the title \"constable\" might be largely ceremonial.\n",
            "\n",
            "To know definitively whether a constable in your location can arrest you, you need to research the laws of your specific jurisdiction.  You can usually find this information online through your local government's website or by consulting legal resources.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed68e216"
      },
      "source": [
        "## Refine fallback logic\n",
        "\n",
        "### Subtask:\n",
        "Refine the fallback logic to the direct LLM by developing more sophisticated criteria for triggering it and logging instances of fallback.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21017f86"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement semantic similarity check and logging for the fallback logic, following the instructions to refine the fallback criteria and track fallback instances.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "8258d15d",
        "outputId": "61c1f0e8-cad5-4291-993c-8e2afa4a662e"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Initialize a dictionary to store fallback logs\n",
        "fallback_logs = {\n",
        "    \"vague_answer\": 0,\n",
        "    \"no_source_documents\": 0,\n",
        "    \"low_semantic_similarity\": 0,\n",
        "    \"rag_error\": 0,\n",
        "    \"pdf_processing_error\": 0\n",
        "}\n",
        "\n",
        "# Define the semantic similarity threshold\n",
        "SEMANTIC_SIMILARITY_THRESHOLD = 0.75 # This can be adjusted based on experimentation\n",
        "\n",
        "def ask(query):\n",
        "    raw_text = \"\"\n",
        "    global fallback_logs # Access the global fallback_logs dictionary\n",
        "    try:\n",
        "        # Attempt to load and process the PDF\n",
        "        reader = PdfReader(PDF_PATH)\n",
        "        raw_text = ''.join(page.extract_text() for page in reader.pages if page.extract_text())\n",
        "\n",
        "        if not raw_text:\n",
        "             raise ValueError(\"Could not extract text from PDF.\")\n",
        "\n",
        "        # Use RecursiveCharacterTextSplitter\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "        texts = text_splitter.split_text(raw_text)\n",
        "\n",
        "        if not texts:\n",
        "            raise ValueError(\"Could not split text into chunks.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: PDF file not found at {PDF_PATH}. Attempting direct LLM fallback.\")\n",
        "        fallback_logs[\"pdf_processing_error\"] += 1\n",
        "        raw_text = None # Indicate that PDF processing failed\n",
        "    except Exception as e:\n",
        "        print(f\"Error during PDF processing: {e}. Attempting direct LLM fallback.\")\n",
        "        fallback_logs[\"pdf_processing_error\"] += 1\n",
        "        raw_text = None # Indicate that PDF processing failed\n",
        "\n",
        "\n",
        "    # Attempt RAG if PDF processing was successful\n",
        "    if raw_text is not None:\n",
        "        try:\n",
        "            embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "            vectorstore = FAISS.from_texts(texts, embedding=embeddings)\n",
        "            # Use the MMR retriever configured outside the function\n",
        "            retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={'k': 5, 'fetch_k': 10})\n",
        "            qa_chain = RetrievalQA.from_chain_type(\n",
        "                llm=llm,\n",
        "                chain_type=\"stuff\",\n",
        "                retriever=retriever, # Use the configured retriever\n",
        "                return_source_documents=True\n",
        "            )\n",
        "\n",
        "            result = qa_chain({\"query\": query})\n",
        "            answer = result[\"result\"].strip()\n",
        "            source_documents = result[\"source_documents\"]\n",
        "\n",
        "            # Check if answer is vague or clearly a hallucination\n",
        "            hallucination_clues = [\n",
        "                \"i don't know\", \"i cannot provide\", \"depends on the jurisdiction\",\n",
        "                \"laws vary\", \"not specified\", \"not clear\", \"no specific article\",\n",
        "                \"as an ai\", \"i'm unable\", \"could not determine\", \"unclear\"\n",
        "            ]\n",
        "            vague_answer = not answer or any(clue in answer.lower() for clue in hallucination_clues)\n",
        "            no_sources = not source_documents\n",
        "\n",
        "            # Calculate semantic similarity between query and RAG answer\n",
        "            try:\n",
        "                query_embedding = embeddings.embed_query(query)\n",
        "                answer_embedding = embeddings.embed_query(answer)\n",
        "                semantic_similarity = cosine_similarity([query_embedding], [answer_embedding])[0][0]\n",
        "                low_similarity = semantic_similarity < SEMANTIC_SIMILARITY_THRESHOLD\n",
        "                print(f\"Semantic similarity: {semantic_similarity:.4f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating semantic similarity: {e}\")\n",
        "                semantic_similarity = -1 # Indicate calculation failed, don't trigger fallback based on this\n",
        "                low_similarity = False # Don't trigger fallback if calculation fails\n",
        "\n",
        "\n",
        "            fallback_needed = False\n",
        "            fallback_reason = \"\"\n",
        "\n",
        "            if vague_answer:\n",
        "                fallback_needed = True\n",
        "                fallback_reason = \"Vague/hallucinated answer\"\n",
        "                fallback_logs[\"vague_answer\"] += 1\n",
        "            elif no_sources:\n",
        "                fallback_needed = True\n",
        "                fallback_reason = \"No source documents\"\n",
        "                fallback_logs[\"no_source_documents\"] += 1\n",
        "            elif low_similarity and semantic_similarity != -1: # Only trigger if similarity calculation was successful\n",
        "                 fallback_needed = True\n",
        "                 fallback_reason = f\"Low semantic similarity ({semantic_similarity:.4f})\"\n",
        "                 fallback_logs[\"low_semantic_similarity\"] += 1\n",
        "\n",
        "\n",
        "            if fallback_needed:\n",
        "                print(f\"Fallback triggered: {fallback_reason}. Attempting direct LLM fallback.\")\n",
        "                raise ValueError(f\"Fallback triggered: {fallback_reason}\")\n",
        "\n",
        "\n",
        "            # Extract article numbers (if found)\n",
        "            articles = []\n",
        "            for doc in source_documents:\n",
        "                matches = re.findall(r'Article\\s*(\\d+)', doc.page_content)\n",
        "                articles.extend(matches)\n",
        "\n",
        "            response = f\"üìò *Answer from Constitution PDF:*\\n{answer}\\n\\n\"\n",
        "            if articles:\n",
        "                response += f\"üìå *Supporting Article(s)*: {', '.join(sorted(set(articles)))}\"\n",
        "            else:\n",
        "                response += \"üìå *Supporting Article(s)*: Could not determine specific article.\"\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during RAG process: {e}. Attempting direct LLM fallback.\")\n",
        "            fallback_logs[\"rag_error\"] += 1\n",
        "\n",
        "\n",
        "    # Fallback to Gemini LLM directly if RAG failed or PDF processing failed\n",
        "    try:\n",
        "        response = model.generate_content(query)\n",
        "        return f\"üß† *Answer from Gemini (LLM):*\\n{response.text.strip()}\"\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Both RAG and Gemini LLM failed: {e}\"\n",
        "\n",
        "# Run some queries to test the updated fallback logic and logging\n",
        "print(ask(\"What is the capital of France?\")) # Should trigger fallback due to irrelevance\n",
        "print(\"-\" * 20)\n",
        "print(ask(\"can a constable arrest me?\")) # Should trigger fallback based on previous runs or low similarity\n",
        "print(\"-\" * 20)\n",
        "print(ask(\"What are the fundamental rights mentioned in the constitution?\")) # Should ideally use RAG\n",
        "print(\"-\" * 20)\n",
        "print(\"Fallback Logs:\", fallback_logs)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic similarity: 0.5666\n",
            "Fallback triggered: Vague/hallucinated answer. Attempting direct LLM fallback.\n",
            "Error during RAG process: Fallback triggered: Vague/hallucinated answer. Attempting direct LLM fallback.\n",
            "üß† *Answer from Gemini (LLM):*\n",
            "Paris\n",
            "--------------------\n",
            "Semantic similarity: 0.8338\n",
            "Fallback triggered: Vague/hallucinated answer. Attempting direct LLM fallback.\n",
            "Error during RAG process: Fallback triggered: Vague/hallucinated answer. Attempting direct LLM fallback.\n",
            "üß† *Answer from Gemini (LLM):*\n",
            "Yes, a constable can arrest you, but their powers vary depending on their jurisdiction and the specific circumstances.  Constables typically have powers of arrest similar to, or sometimes less than, police officers.  The exact extent of their powers will depend on the laws of the specific state, province, or country where they operate.  Their authority is often limited to specific areas or types of offenses.  For example, some constables may primarily serve court documents and have limited arrest powers, while others might have more extensive powers, especially in rural areas.\n",
            "\n",
            "If a constable arrests you, you have the right to know why and to be treated fairly according to the law.  You should ask to see their identification and inquire about the reason for your arrest.\n",
            "\n",
            "**In short:  It's possible, but the specifics depend on where you are.** If you're concerned about a specific situation, you should consult with a legal professional in your area.\n",
            "--------------------\n",
            "Semantic similarity: 0.8079\n",
            "üìò *Answer from Constitution PDF:*\n",
            "Based on the provided text, the fundamental rights mentioned include:\n",
            "\n",
            "*   Right to Equality (Articles 14-17)\n",
            "*   Right to Constitutional Remedies (Article 32)\n",
            "*   Protection of life and personal liberty (Article 21)\n",
            "*   Right to education (Article 21A)\n",
            "*   Freedom of religion (Articles 25-28, partially shown)\n",
            "\n",
            "The text is incomplete, so this list may not be exhaustive.\n",
            "\n",
            "üìå *Supporting Article(s)*: Could not determine specific article.\n",
            "--------------------\n",
            "Fallback Logs: {'vague_answer': 2, 'no_source_documents': 0, 'low_semantic_similarity': 0, 'rag_error': 2, 'pdf_processing_error': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa46c6a5"
      },
      "source": [
        "## Add conversation memory\n",
        "\n",
        "### Subtask:\n",
        "Integrate memory into the LangChain `RetrievalQA` chain to enable multi-turn conversations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe6435f0"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary memory component and modify the `qa_chain` initialization to include memory, preparing to update the `ask` function for multi-turn conversation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "101f9a44",
        "outputId": "92497cd4-a09d-4c95-a91a-a02f7aead819"
      },
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Define the semantic similarity threshold\n",
        "SEMANTIC_SIMILARITY_THRESHOLD = 0.75 # This can be adjusted based on experimentation\n",
        "\n",
        "# Initialize a dictionary to store fallback logs\n",
        "fallback_logs = {\n",
        "    \"vague_answer\": 0,\n",
        "    \"no_source_documents\": 0,\n",
        "    \"low_semantic_similarity\": 0,\n",
        "    \"rag_error\": 0,\n",
        "    \"pdf_processing_error\": 0\n",
        "}\n",
        "\n",
        "# Initialize ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# Function to initialize or get the RAG chain with memory\n",
        "def get_rag_chain():\n",
        "    raw_text = \"\"\n",
        "    try:\n",
        "        # Attempt to load and process the PDF\n",
        "        reader = PdfReader(PDF_PATH)\n",
        "        raw_text = ''.join(page.extract_text() for page in reader.pages if page.extract_text())\n",
        "\n",
        "        if not raw_text:\n",
        "             raise ValueError(\"Could not extract text from PDF.\")\n",
        "\n",
        "        # Use RecursiveCharacterTextSplitter\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "        texts = text_splitter.split_text(raw_text)\n",
        "\n",
        "        if not texts:\n",
        "            raise ValueError(\"Could not split text into chunks.\")\n",
        "\n",
        "        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "        vectorstore = FAISS.from_texts(texts, embedding=embeddings)\n",
        "        retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={'k': 5, 'fetch_k': 10})\n",
        "\n",
        "        # Create the RetrievalQA chain with memory\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=retriever,\n",
        "            return_source_documents=True,\n",
        "            memory=memory # Pass the memory object here\n",
        "        )\n",
        "        return qa_chain\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during RAG chain setup: {e}\")\n",
        "        return None\n",
        "\n",
        "# Initialize the RAG chain globally or manage its state to persist memory\n",
        "# For simplicity in this example, we will re-initialize it if needed, but for a long-running app,\n",
        "# the chain and memory would need to be managed persistently.\n",
        "qa_chain_with_memory = get_rag_chain()\n",
        "\n",
        "# Update the ask function to use the chain with memory and invoke method\n",
        "def ask(query):\n",
        "    global fallback_logs\n",
        "    global qa_chain_with_memory # Access the global chain with memory\n",
        "\n",
        "    if qa_chain_with_memory is None:\n",
        "        print(\"RAG chain could not be initialized. Falling back to direct LLM.\")\n",
        "        # Fallback to Gemini LLM directly if RAG chain setup failed\n",
        "        try:\n",
        "            response = model.generate_content(query)\n",
        "            return f\"üß† *Answer from Gemini (LLM):*\\n{response.text.strip()}\"\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Both RAG and Gemini LLM failed: {e}\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Use the invoke method with the query\n",
        "        result = qa_chain_with_memory.invoke({\"query\": query})\n",
        "        answer = result[\"result\"].strip()\n",
        "        source_documents = result[\"source_documents\"]\n",
        "\n",
        "        # Check if answer is vague or clearly a hallucination\n",
        "        hallucination_clues = [\n",
        "            \"i don't know\", \"i cannot provide\", \"depends on the jurisdiction\",\n",
        "            \"laws vary\", \"not specified\", \"not clear\", \"no specific article\",\n",
        "            \"as an ai\", \"i'm unable\", \"could not determine\", \"unclear\"\n",
        "        ]\n",
        "        vague_answer = not answer or any(clue in answer.lower() for clue in hallucination_clues)\n",
        "        no_sources = not source_documents\n",
        "\n",
        "        # Calculate semantic similarity between query and RAG answer\n",
        "        semantic_similarity = -1 # Default to -1 in case of error\n",
        "        low_similarity = False\n",
        "        try:\n",
        "            embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "            query_embedding = embeddings.embed_query(query)\n",
        "            # Ensure the answer is not empty before embedding\n",
        "            if answer:\n",
        "                answer_embedding = embeddings.embed_query(answer)\n",
        "                semantic_similarity = cosine_similarity([query_embedding], [answer_embedding])[0][0]\n",
        "                low_similarity = semantic_similarity < SEMANTIC_SIMILARITY_THRESHOLD\n",
        "                print(f\"Semantic similarity: {semantic_similarity:.4f}\")\n",
        "            else:\n",
        "                 print(\"RAG answer is empty, skipping semantic similarity calculation.\")\n",
        "                 low_similarity = True # Consider empty answer as low similarity for fallback\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating semantic similarity: {e}\")\n",
        "            semantic_similarity = -1 # Indicate calculation failed, don't trigger fallback based on this\n",
        "            low_similarity = False # Don't trigger fallback if calculation fails\n",
        "\n",
        "\n",
        "        fallback_needed = False\n",
        "        fallback_reason = \"\"\n",
        "\n",
        "        if vague_answer:\n",
        "            fallback_needed = True\n",
        "            fallback_reason = \"Vague/hallucinated answer\"\n",
        "            fallback_logs[\"vague_answer\"] += 1\n",
        "        elif no_sources:\n",
        "            fallback_needed = True\n",
        "            fallback_reason = \"No source documents\"\n",
        "            fallback_logs[\"no_source_documents\"] += 1\n",
        "        elif low_similarity and semantic_similarity != -1: # Only trigger if similarity calculation was successful\n",
        "             fallback_needed = True\n",
        "             fallback_reason = f\"Low semantic similarity ({semantic_similarity:.4f})\"\n",
        "             fallback_logs[\"low_semantic_similarity\"] += 1\n",
        "\n",
        "\n",
        "        if fallback_needed:\n",
        "            print(f\"Fallback triggered: {fallback_reason}. Attempting direct LLM fallback.\")\n",
        "            raise ValueError(f\"Fallback triggered: {fallback_reason}\")\n",
        "\n",
        "\n",
        "        # Extract article numbers (if found)\n",
        "        articles = []\n",
        "        for doc in source_documents:\n",
        "            matches = re.findall(r'Article\\s*(\\d+)', doc.page_content)\n",
        "            articles.extend(matches)\n",
        "\n",
        "        response = f\"üìò *Answer from Constitution PDF:*\\n{answer}\\n\\n\"\n",
        "        if articles:\n",
        "            response += f\"üìå *Supporting Article(s)*: {', '.join(sorted(set(articles)))}\"\n",
        "        else:\n",
        "            response += \"üìå *Supporting Article(s)*: Could not determine specific article.\"\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during RAG process: {e}. Attempting direct LLM fallback.\")\n",
        "        fallback_logs[\"rag_error\"] += 1\n",
        "        # Fallback to Gemini LLM directly if RAG failed\n",
        "        try:\n",
        "            response = model.generate_content(query)\n",
        "            return f\"üß† *Answer from Gemini (LLM):*\\n{response.text.strip()}\"\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Both RAG and Gemini LLM failed: {e}\"\n",
        "\n",
        "\n",
        "# Test the updated system with a sequence of related questions\n",
        "print(ask(\"What are the fundamental rights mentioned in the constitution?\"))\n",
        "print(\"-\" * 20)\n",
        "print(ask(\"Can these rights be suspended?\"))\n",
        "print(\"-\" * 20)\n",
        "print(ask(\"What about during an emergency?\"))\n",
        "print(\"-\" * 20)\n",
        "print(\"Fallback Logs:\", fallback_logs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2755775166.py:16: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during RAG process: Got multiple output keys: dict_keys(['result', 'source_documents']), cannot determine which to store in memory. Please set the 'output_key' explicitly.. Attempting direct LLM fallback.\n",
            "üß† *Answer from Gemini (LLM):*\n",
            "The fundamental rights in the Indian Constitution are enshrined in **Part III (Articles 12-35)**.  They are:\n",
            "\n",
            "1. **Right to Equality (Articles 14-18):** This guarantees equality before the law, prohibition of discrimination on grounds of religion, race, caste, sex or place of birth, equality of opportunity in matters of public employment, and abolition of titles.\n",
            "\n",
            "2. **Right to Freedom (Articles 19-22):** This includes freedom of speech and expression, assembly, association, movement, residence, and profession or occupation.  However, these freedoms are subject to reasonable restrictions imposed by law.  It also includes protection against arbitrary arrest and detention.\n",
            "\n",
            "3. **Right against Exploitation (Articles 23-24):** This prohibits traffic in human beings and forced labour, and also prohibits child labour.\n",
            "\n",
            "4. **Right to Freedom of Religion (Articles 25-28):** This guarantees freedom of conscience and free profession, practice, and propagation of religion.  It also ensures freedom to manage religious affairs and prohibits the state from establishing a religion.\n",
            "\n",
            "5. **Cultural and Educational Rights (Articles 29-30):** This protects the rights of minorities to conserve their distinct language, script, and culture.  It also gives minorities the right to establish and administer educational institutions of their choice.\n",
            "\n",
            "6. **Right to Constitutional Remedies (Article 32):** This is considered the heart and soul of fundamental rights.  It guarantees the right to move the Supreme Court for the enforcement of fundamental rights.  This right is itself a fundamental right.\n",
            "\n",
            "\n",
            "It's important to note that these rights are not absolute and can be subject to reasonable restrictions in the interest of public order, morality, health, etc.  The Supreme Court of India plays a crucial role in interpreting and protecting these fundamental rights.\n",
            "--------------------\n",
            "Error during RAG process: Got multiple output keys: dict_keys(['result', 'source_documents']), cannot determine which to store in memory. Please set the 'output_key' explicitly.. Attempting direct LLM fallback.\n",
            "üß† *Answer from Gemini (LLM):*\n",
            "Whether rights can be suspended depends entirely on *which* rights you're referring to and *under what circumstances*.  There's no universal answer.\n",
            "\n",
            "Some rights, considered fundamental or inalienable, are generally understood to be extremely difficult, if not impossible, to suspend completely, even in times of emergency.  Examples might include:\n",
            "\n",
            "* **The right to life:** While capital punishment exists in some places, even that is a highly regulated and legally scrutinized process.  Mass, indiscriminate killing is universally condemned.\n",
            "* **The right to be free from torture:**  International human rights law prohibits torture under almost all circumstances.\n",
            "\n",
            "Other rights, however, may be subject to limitations or suspension under specific, carefully defined conditions, usually involving a compelling state interest and strict procedural safeguards.  These might include:\n",
            "\n",
            "* **Freedom of speech:**  This can be limited in cases involving incitement to violence, defamation, or revealing state secrets.\n",
            "* **Freedom of assembly:**  Large gatherings can be restricted during public health emergencies or to prevent violence.\n",
            "* **Right to privacy:**  Law enforcement may be able to obtain warrants for surveillance in specific circumstances.\n",
            "* **Right to due process:**  While the right to a fair trial is fundamental, certain aspects might be adjusted in wartime or under states of emergency (though this is highly controversial and requires strict legal justification).\n",
            "* **Right to travel:**  This can be restricted during pandemics or for reasons of national security.\n",
            "\n",
            "\n",
            "The specific legal framework governing the suspension of rights varies significantly by country and jurisdiction.  International human rights law sets minimum standards, but the implementation and interpretation of these standards differ.  Often, there are judicial mechanisms for challenging the suspension of rights.\n",
            "\n",
            "To answer your question accurately, you need to specify the rights you are asking about and the legal and political context.\n",
            "--------------------\n",
            "Error during RAG process: Got multiple output keys: dict_keys(['result', 'source_documents']), cannot determine which to store in memory. Please set the 'output_key' explicitly.. Attempting direct LLM fallback.\n",
            "üß† *Answer from Gemini (LLM):*\n",
            "Please provide me with context.  \"During an emergency\" is very broad.  To answer your question, I need to know *what kind* of emergency you're referring to.  For example:\n",
            "\n",
            "* **Medical emergency?**  (e.g., heart attack, stroke, severe bleeding)  In this case, call emergency services immediately (911 in the US, or your local equivalent).\n",
            "* **Natural disaster?** (e.g., earthquake, hurricane, wildfire)  Follow evacuation orders, secure your home, and seek shelter.\n",
            "* **Home emergency?** (e.g., fire, gas leak, power outage)  Evacuate if necessary, call emergency services, and take appropriate safety precautions.\n",
            "* **Personal safety emergency?** (e.g., being attacked, witnessing a crime)  Call emergency services, try to get to safety, and if possible, document what happened.\n",
            "\n",
            "\n",
            "Give me more information, and I can provide a more specific and helpful answer.\n",
            "--------------------\n",
            "Fallback Logs: {'vague_answer': 0, 'no_source_documents': 0, 'low_semantic_similarity': 0, 'rag_error': 3, 'pdf_processing_error': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z9893t8rfO9R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}